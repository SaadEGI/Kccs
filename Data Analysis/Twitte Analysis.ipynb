{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "news= pd.read_csv('new.csv')\n",
    "news.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "news['tweet'].str.len().hist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "news['tweet'].str.split().\\\n",
    "   apply(lambda x : [len(i) for i in x]). \\\n",
    "   map(lambda x: np.mean(x)).hist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop=set(stopwords.words('english'))\n",
    "corpus=[]\n",
    "new= news['tweet'].str.split()\n",
    "new=new.values.tolist()\n",
    "corpus=[word for i in new for word in i]\n",
    "\n",
    "from collections import defaultdict\n",
    "dic=defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop:\n",
    "        dic[word]+=1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:15]\n",
    "x,y=zip(*top)\n",
    "plt.bar(x,y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Here you can see that there are some useless words that are being used, we can remove them for cleaner data**\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "counter=Counter(corpus)\n",
    "most=counter.most_common()\n",
    "\n",
    "x, y= [], []\n",
    "for word,count in most[:40]:\n",
    "    if (word not in stop):\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "\n",
    "sns.barplot(x=y,y=x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_ngram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx])\n",
    "                  for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " top_n_bigrams=get_top_ngram(news['tweet'],2)[:10]\n",
    " x,y=map(list,zip(*top_n_bigrams))\n",
    " sns.barplot(x=y,y=x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "top_tri_grams=get_top_ngram(news['tweet'],n=3)\n",
    "x,y=map(list,zip(*top_tri_grams))\n",
    "sns.barplot(x=y,y=x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pyLDAvis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_news(df):\n",
    "    corpus=[]\n",
    "    stem=PorterStemmer()\n",
    "    lem=WordNetLemmatizer()\n",
    "    for news in df['tweet']:\n",
    "        words=[w for w in word_tokenize(news) if (w not in stop)]\n",
    "\n",
    "        words=[lem.lemmatize(w) for w in words if len(w)>2]\n",
    "\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus=preprocess_news(news)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dic=gensim.corpora.Dictionary(corpus)\n",
    "bow_corpus = [dic.doc2bow(doc) for doc in corpus]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus,\n",
    "                                   num_topics = 4,\n",
    "                                   id2word = dic,\n",
    "                                   passes = 10,\n",
    "                                   workers = 2)\n",
    "lda_model.show_topics()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, bow_corpus, dic)\n",
    "vis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "def show_wordcloud(data):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=100,\n",
    "        max_font_size=30,\n",
    "        scale=3,\n",
    "        random_state=1)\n",
    "\n",
    "    wordcloud=wordcloud.generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "show_wordcloud(corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Named entity recognition Step**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "doc=nlp('India and Iran have agreed to boost the economic viability \\\n",
    "of the strategic Chabahar port through various measures, \\\n",
    "including larger subsidies to merchant shipping firms using the facility, \\\n",
    "people familiar with the development said on Thursday.')\n",
    "\n",
    "[(x.text,x.label_) for x in doc.ents]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Application on Data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ner(text):\n",
    "    doc=nlp(text)\n",
    "    return [X.label_ for X in doc.ents]\n",
    "\n",
    "ent=news['fulltext'].\\\n",
    "    apply(lambda x : ner(x))\n",
    "ent=[x for sub in ent for x in sub]\n",
    "\n",
    "counter=Counter(ent)\n",
    "count=counter.most_common()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x,y=map(list,zip(*count))\n",
    "sns.barplot(x=y,y=x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ner(text,ent=\"GPE\"):\n",
    "    doc=nlp(text)\n",
    "    return [X.text for X in doc.ents if X.label_ == ent]\n",
    "\n",
    "gpe=news['fulltext'].apply(lambda x: ner(x))\n",
    "gpe=[i for x in gpe for i in x]\n",
    "counter=Counter(gpe)\n",
    "\n",
    "x,y=map(list,zip(*counter.most_common(10)))\n",
    "sns.barplot(y,x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "per=news['fulltext'].apply(lambda x: ner(x,\"PERSON\"))\n",
    "per=[i for x in per for i in x]\n",
    "counter=Counter(per)\n",
    "\n",
    "x,y=map(list,zip(*counter.most_common(10)))\n",
    "sns.barplot(y,x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "org=news['fulltext'].apply(lambda x: ner(x,\"ORG\"))\n",
    "org=[i for x in org for i in x]\n",
    "counter=Counter(org)\n",
    "\n",
    "x,y=map(list,zip(*counter.most_common(10)))\n",
    "sns.barplot(y,x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}