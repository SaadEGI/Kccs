on:
 schedule:
  #  - cron:  '*/5 * * * *' # every 5 minutes
   - cron: '0,5,10,15,20,25,30,35,40,45,50,55 * * * *' # every 5 minutes fr
name: Scrape Articles
jobs:
  build:
    name: Build
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@master
    # - name: prepare
      # run: |
      #  git clone https://github.com/tasos-py/Search-Engines-Scraper;
      #  cd Search-Engines-Scraper;
      #  sudo python setup.py install;
      # run: |
    - name: Scrape articles 
      # run: |
      #  cd Search-Engines-Scraper;
      #  python search_engines_cli.py -e google -q "%22corona%22 site:nytimes.com &tbs=qdr:w" -o json,print -p 2
      run: |
          cd Crawlers/Newspapers;
          sudo python3 Action.py 1;
    # - name: clean
    #   run: |
    #    sudo mv /home/runner/work/Kccs/Kccs/Search-Engines-Scraper/search_engines/search_results/output.json /home/runner/work/Kccs/Kccs/output.json;
    #    sudo rm -r /home/runner/work/Kccs/Kccs/Search-Engines-Scraper;
    - uses: mikeal/publish-to-github-action@master
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        BRANCH_NAME: 'main' 
# TODO: speed process by avoiding cloning and installing each time the action is run
# TODO: instead use the library in an action script

