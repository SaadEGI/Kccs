on:
 schedule:
  #  - cron:  '*/5 * * * *' # every 5 minutes
   - cron: '0,5,10,15,20,25,30,35,40,45,50,55 * * * *' # every 5 minutes fr
 workflow_dispatch:
  inputs:
    logLevel:
      description: 'Log level'     
      required: false
      default: 'warning'
name: Scrape Articles
jobs:
  build:
    name: Build
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@master
    - name: prepare
      run: |
       cd Crawlers/Newspapers/Search_Engines_Scraper
       sudo python setup.py install;
       sudo pip3 install fake_useragent numpy;
      #  cd Search-Engines-Scraper;
      # run: |
    - name: Scrape articles 
      # run: |
      #  cd Search-Engines-Scraper;
      #  python search_engines_cli.py -e google -q "%22corona%22 site:nytimes.com &tbs=qdr:w" -o json,print -p 2
      run: |
          cd Crawlers/Newspapers;
          sudo python3 Action.py 1.0;
    # - name: clean
    #   run: |
    #    sudo mv /home/runner/work/Kccs/Kccs/Search-Engines-Scraper/search_engines/search_results/output.json /home/runner/work/Kccs/Kccs/output.json;
    #    sudo rm -r /home/runner/work/Kccs/Kccs/Search-Engines-Scraper;
    - uses: mikeal/publish-to-github-action@master
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        BRANCH_NAME: 'main' 
# TODO: speed process by avoiding cloning and installing each time the action is run
# TODO: instead use the library in an action script

